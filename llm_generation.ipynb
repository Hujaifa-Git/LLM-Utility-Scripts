{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Generation Tenchines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Environmental Stetup & Import\n",
    "Install necessary packages and import them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers torch bitsandbytes flash-attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nsl3090-3/anaconda3/envs/langchain2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Load Model & Tokenizer\n",
    "You can use any Huggaingface model by providing the path here. Quantization and Flash Attention is enalbled here. You can turn them off if you want.\n",
    "Tokenizers padding side should always be on the *left*. Many LLM doesn't have a padding token so padding token needs to be set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.85s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"teknium/OpenHermes-2.5-Mistral-7B\", #model path\n",
    "     device_map=\"auto\", #device mapping \n",
    "     load_in_4bit=True, #quantization\n",
    "     attn_implementation=\"flash_attention_2\", #Flash Attention\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"teknium/OpenHermes-2.5-Mistral-7B\", padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token #Setting pad token\n",
    "tokenizer.padding_side='left'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Tokenize Input (Using Tokenizer)\n",
    "\n",
    "Use Tokenizer to tokenize your input. If you want to tokenize a batch of input, their length have to be equal. for this reason padding is required. \"return_tensors\" make sure the tokes are in tensors and if they are then they need to be setted in a device. Tokenizer returns \"input_ids\" (tokens) and \"attention_mask\". attention_mask=0 at the padded tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,   330,  1274,   302,  9304, 28747,  2760, 28725,  5045],\n",
       "        [32000, 32000, 32000, 32000,     1,  1824,   349,   574,  1141]],\n",
       "       device='cuda:1'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 1, 1, 1, 1, 1]], device='cuda:1')}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs = tokenizer([\"A list of colors: red, blue\", 'What is your name'], return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "input_length = model_inputs['input_ids'].shape[1]\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['A list of colors: red, blue, green, yellow, orange, purple, pink, black, white, brown, gray, silver, gold, tan, beige, maroon, navy, teal, olive, lime, chartreuse, magenta, fuch',\n",
       " 'A list of colors: red, blue, green, yellow, orange, purple, pink, black, white, brown, gray, silver, gold, tan, beige, maroon, navy, teal, olive, lime, chartreuse, turquoise, mag',\n",
       " 'What is your name and role/job title?\\n\\nMy name is Katie and I’m a freelance writer, editor, and content creator. I specialize in health and wellness content, but I also write about a variety of other topics,',\n",
       " 'What is your name and role/job title?\\n\\nMy name is Katie and I’m a freelance writer, editor, and content creator. I specialize in health and wellness content, but I also write about a variety of other topics.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#More about generation config and decoding is given below\n",
    "generated_ids = model.generate(**model_inputs, \n",
    "                               max_new_tokens=50, \n",
    "                               num_beams = 5, \n",
    "                               early_stopping = True, \n",
    "                               no_repeat_ngram_size=2, \n",
    "                               num_return_sequences=2, \n",
    "                               top_k = 40, \n",
    "                               do_sample = True, \n",
    "                               temperature = 0.5,\n",
    "                               top_p = 0.9 \n",
    "                               )\n",
    "result =  tokenizer.batch_decode(generated_ids[:,input_length:], skip_special_tokens=True)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: Tokenizer Input (Using Chat_Template)\n",
    "Using chat template to tokenize input is always the best option as it includes the necessary tags for the model to understand the prompt better. Model usually works without these tags but you can get better perfomance by following the chat_template because the model was trained using the format of chat template.\n",
    "At first you can print to see the current chat_templat and then make a input dictionary that follows the template.\n",
    "Chat_templates are written in Jinja format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '<|im_end|>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|> system\n",
      "You are a friendly chatbot who always responds in the style of rapper like g-eazy or snoop dogg<|im_end|> \n",
      "<|im_start|> user\n",
      "How many helicopters can a human eat in one sitting?<|im_end|> \n",
      "<|im_start|> assistant\n",
      "Yo, dude, I ain't no chef or somethin', but I think you gotta be kiddin' me, right? You ain't gonna eat no helicopter, period. Maybe if you go to some fancy-pants restaurant, they put a toy helicopter on the plate, but that don't count, homie. So my answer is zero, you won't be eatin' no helicopter in one sitting, no way.<|im_end|> \n",
      "<|im_start|> user\n",
      "What if I want to eat it<|im_end|> \n",
      "<|im_start|> assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#This template contains 'role' and 'content' that's why these are used here. May differ if the chat_template is different\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of rapper like g-eazy or snoop dogg\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    "    {'role':'assistant', 'content':\"Yo, dude, I ain't no chef or somethin', but I think you gotta be kiddin' me, right? You ain't gonna eat no helicopter, period. Maybe if you go to some fancy-pants restaurant, they put a toy helicopter on the plate, but that don't count, homie. So my answer is zero, you won't be eatin' no helicopter in one sitting, no way.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What if I want to eat it\"}\n",
    "]\n",
    "model_inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "input_length = model_inputs.shape[1] #Get the token lenght of input so that we can filter that part out from the final response\n",
    "print(tokenizer.batch_decode(model_inputs)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5: Generation Config\n",
    "There are many hyper-parameter in generation config which can be tuned to get the desired result. Some of the most used hyper parameters are listed here\n",
    "<li>model_inputs [Contains input_ids (tokens) & attention_mask]</li>\n",
    "<li>num_beams [It enables beam search, reduce repitition. 'X' is the number of beams/ possible future token to consider before selecting the next token. May reduce creativity and be more deterministic. If not used then the model used greedy method to just select the most probable token]</li>\n",
    "<li>early_stopping [Controls the stopping condition for beam-based methods, like beam-search.]</li>\n",
    "<li>no_repeat_ngram_size [model checks the previous 'X' tokens and make sure the next generated token is not the same as any of the previous \"X\" Token] </li>\n",
    "<li>num_return_sequences [how many output seqence the model will generate for each input]</li>\n",
    "<li>do_sample [True = randomly pic a token among the top_k probable token which has valuse larger than top_p. False = pick highest probably token only]</li>\n",
    "<li>top_k [number of probable next token to consider. 0 means full vocabulary]</li>\n",
    "<li>top_p [top_p' limits the number of probable token by filtering tokens whose probability values are greater than 'x'. top_p = 1 means it considers the entire vocabulary]</li>\n",
    "<li>temperature [By lowering temperature we can increasing the likelihood of high probability words and decreasing the likelihood of low probability words (more deterministic). By increasing we can decrease the likelihood of high probability words and increase the likelihood of low probability words (more creative)]</li>\n",
    "Only the most used ones are listed here more can be found with detailed explanation in Huggingface Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/nsl3090-3/anaconda3/envs/langchain2/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:228: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.')\n"
     ]
    }
   ],
   "source": [
    "generated_ids = model.generate(model_inputs, \n",
    "                               max_new_tokens=50, \n",
    "                               num_beams = 5, \n",
    "                               early_stopping = True, \n",
    "                               no_repeat_ngram_size=2, \n",
    "                               num_return_sequences=2, \n",
    "                               top_k = 40, \n",
    "                               do_sample = True, \n",
    "                               temperature = 0.5,\n",
    "                               top_p = 0.9 \n",
    "                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6: Decoding and Stripping\n",
    "The model gives tokens as their output from 'generate()'. This token also includes the initial input tokens. So we need to remove input tokens and decode the token values to gwt our final response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hold up, my man, let me break it down for you. You wanna eat a helicoptah, huh? That's some next-level stuff right there. But lemme tell ya, chopper-munchin\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)\n",
    "print(output[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
